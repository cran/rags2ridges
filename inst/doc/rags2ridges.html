<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />



<title>Introduction to rags2ridges</title>

<script>// Hide empty <a> tag within highlighted CodeBlock for screen reader accessibility (see https://github.com/jgm/pandoc/issues/6352#issuecomment-626106786) -->
// v0.0.1
// Written by JooYoung Seo (jooyoung@psu.edu) and Atsushi Yasumoto on June 1st, 2020.

document.addEventListener('DOMContentLoaded', function() {
  const codeList = document.getElementsByClassName("sourceCode");
  for (var i = 0; i < codeList.length; i++) {
    var linkList = codeList[i].getElementsByTagName('a');
    for (var j = 0; j < linkList.length; j++) {
      if (linkList[j].innerHTML === "") {
        linkList[j].setAttribute('aria-hidden', 'true');
      }
    }
  }
});
</script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>


<style type="text/css">
  code {
    white-space: pre;
  }
  .sourceCode {
    overflow: visible;
  }
</style>
<style type="text/css" data-origin="pandoc">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */

</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    for (var j = 0; j < rules.length; j++) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") continue;
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' && rule.style.backgroundColor === '') continue;
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>




<style type="text/css">body {
background-color: #fff;
margin: 1em auto;
max-width: 700px;
overflow: visible;
padding-left: 2em;
padding-right: 2em;
font-family: "Open Sans", "Helvetica Neue", Helvetica, Arial, sans-serif;
font-size: 14px;
line-height: 1.35;
}
#TOC {
clear: both;
margin: 0 0 10px 10px;
padding: 4px;
width: 400px;
border: 1px solid #CCCCCC;
border-radius: 5px;
background-color: #f6f6f6;
font-size: 13px;
line-height: 1.3;
}
#TOC .toctitle {
font-weight: bold;
font-size: 15px;
margin-left: 5px;
}
#TOC ul {
padding-left: 40px;
margin-left: -1.5em;
margin-top: 5px;
margin-bottom: 5px;
}
#TOC ul ul {
margin-left: -2em;
}
#TOC li {
line-height: 16px;
}
table {
margin: 1em auto;
border-width: 1px;
border-color: #DDDDDD;
border-style: outset;
border-collapse: collapse;
}
table th {
border-width: 2px;
padding: 5px;
border-style: inset;
}
table td {
border-width: 1px;
border-style: inset;
line-height: 18px;
padding: 5px 5px;
}
table, table th, table td {
border-left-style: none;
border-right-style: none;
}
table thead, table tr.even {
background-color: #f7f7f7;
}
p {
margin: 0.5em 0;
}
blockquote {
background-color: #f6f6f6;
padding: 0.25em 0.75em;
}
hr {
border-style: solid;
border: none;
border-top: 1px solid #777;
margin: 28px 0;
}
dl {
margin-left: 0;
}
dl dd {
margin-bottom: 13px;
margin-left: 13px;
}
dl dt {
font-weight: bold;
}
ul {
margin-top: 0;
}
ul li {
list-style: circle outside;
}
ul ul {
margin-bottom: 0;
}
pre, code {
background-color: #f7f7f7;
border-radius: 3px;
color: #333;
white-space: pre-wrap; 
}
pre {
border-radius: 3px;
margin: 5px 0px 10px 0px;
padding: 10px;
}
pre:not([class]) {
background-color: #f7f7f7;
}
code {
font-family: Consolas, Monaco, 'Courier New', monospace;
font-size: 85%;
}
p > code, li > code {
padding: 2px 0px;
}
div.figure {
text-align: center;
}
img {
background-color: #FFFFFF;
padding: 2px;
border: 1px solid #DDDDDD;
border-radius: 3px;
border: 1px solid #CCCCCC;
margin: 0 5px;
}
h1 {
margin-top: 0;
font-size: 35px;
line-height: 40px;
}
h2 {
border-bottom: 4px solid #f7f7f7;
padding-top: 10px;
padding-bottom: 2px;
font-size: 145%;
}
h3 {
border-bottom: 2px solid #f7f7f7;
padding-top: 10px;
font-size: 120%;
}
h4 {
border-bottom: 1px solid #f7f7f7;
margin-left: 8px;
font-size: 105%;
}
h5, h6 {
border-bottom: 1px solid #ccc;
font-size: 105%;
}
a {
color: #0033dd;
text-decoration: none;
}
a:hover {
color: #6666ff; }
a:visited {
color: #800080; }
a:visited:hover {
color: #BB00BB; }
a[href^="http:"] {
text-decoration: underline; }
a[href^="https:"] {
text-decoration: underline; }

code > span.kw { color: #555; font-weight: bold; } 
code > span.dt { color: #902000; } 
code > span.dv { color: #40a070; } 
code > span.bn { color: #d14; } 
code > span.fl { color: #d14; } 
code > span.ch { color: #d14; } 
code > span.st { color: #d14; } 
code > span.co { color: #888888; font-style: italic; } 
code > span.ot { color: #007020; } 
code > span.al { color: #ff0000; font-weight: bold; } 
code > span.fu { color: #900; font-weight: bold; } 
code > span.er { color: #a61717; background-color: #e3d2d2; } 
</style>




</head>

<body>




<h1 class="title toc-ignore">Introduction to rags2ridges</h1>



<p><strong>rags2ridges</strong> is an R-package for <em>fast</em> and <em>proper</em> L2-penalized estimation of precision (and covariance) matrices also called <strong>ridge estimation</strong>. Its L2-penalty features the ability to shrink towards a target matrix, allowing for incorporation of prior knowledge. Likewise, it also features a <em>fused</em> L2 ridge penalty allows for simultaneous estimation of multiple matrices. The package also contains additional functions for post-processing the L2-penalized estimates — useful for feature selection and when doing graphical modelling. The <em>fused</em> ridge estimation is useful when dealing with grouped data as when doing meta or integrative analysis.</p>
<p>This vignette provides a light introduction on how to get started with regular ridge estimation of precision matrices and further steps.</p>
<div id="getting-started" class="section level2">
<h2>Getting started</h2>
<div id="package-installation" class="section level3">
<h3>Package installation</h3>
<p>The README details how to install the <strong>rags2ridges</strong> package. When installed, the package is loaded as seen below where we also define a function for adding pretty names to a matrix.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1"></a><span class="kw">library</span>(rags2ridges)</span></code></pre></div>
</div>
<div id="small-theoretical-primer-and-package-usage" class="section level3">
<h3>Small theoretical primer and package usage</h3>
<p>The sample variance-covariance matrix, or simply <em>covariance matrix</em>, is well-known and ubiquitous. It is given by</p>
<p><span class="math display">\[
S = \frac{1}{n - 1}XX^T
\]</span></p>
<p>where <span class="math inline">\(X\)</span> is the <span class="math inline">\(n \times p\)</span> data matrix that is zero-centered with each <span class="math inline">\(p\)</span>-dimensional observations in the rows. I.e. each row of <span class="math inline">\(X\)</span> is an observation and each column is feature. Often high-dimensional data is organised this way (or transposed).</p>
<p>That <span class="math inline">\(X\)</span> is zero-centered simply means that the column means has been subtracted the columns. The very similar estimate <span class="math inline">\(S = \frac{1}{n}XX^T\)</span> without <a href="https://en.wikipedia.org/wiki/Bessel%27s_correction">Bessel’s correction</a> is the maximum likelihood estimate in a multivariate normal model with mean <span class="math inline">\(0\)</span> and covariance <span class="math inline">\(\Sigma\)</span>. The likelihood function in this case is given by</p>
<p><span class="math display">\[
\ell(\Omega; S) = \ln|\Omega| - \text{tr}(S\Omega)
\]</span></p>
<p>where <span class="math inline">\(\Omega = \Sigma^{-1}\)</span> is the so-called precision matrix (also sometimes called the <em>concentration matrix</em>). It is precisely this <span class="math inline">\(\Omega\)</span> for which we seek an estimate we will denote <span class="math inline">\(P\)</span>. Indeed, one can naturally try to use the inverse of <span class="math inline">\(S\)</span> for this:</p>
<p><span class="math display">\[
P = S^{-1}
\]</span></p>
<p>Let’s try.</p>
<p>The <code>createS()</code> function can easily simulate covariance matrices. But we go a more verbose route for illustration:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1"></a>p &lt;-<span class="st"> </span><span class="dv">6</span></span>
<span id="cb2-2"><a href="#cb2-2"></a>n &lt;-<span class="st"> </span><span class="dv">20</span></span>
<span id="cb2-3"><a href="#cb2-3"></a>X &lt;-<span class="st"> </span><span class="kw">createS</span>(<span class="dt">n =</span> n, <span class="dt">p =</span> p, <span class="dt">dataset =</span> <span class="ot">TRUE</span>)</span>
<span id="cb2-4"><a href="#cb2-4"></a><span class="kw">head</span>(X, <span class="dt">n =</span> <span class="dv">4</span>) <span class="co"># Show 4 first of the n rows</span></span></code></pre></div>
<pre><code>##           A       B       C       D      E      F
## [1,] -2.355  0.3773  0.9469  1.6677 -0.385 -0.262
## [2,]  0.153 -0.5901  2.0843  0.1033 -0.720 -1.711
## [3,]  0.804  0.0954  0.0858  0.0618  2.118 -0.427
## [4,]  0.344  1.1899 -1.3228 -2.0971  0.335 -2.330</code></pre>
<p>Here the columns corresponds to features A, B, C, and so on.</p>
<p>When can then arrive a the MLE using <code>covML()</code> which <em>centers</em> X (subtracting the column means) and then computes the estimate:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1"></a>S &lt;-<span class="st"> </span><span class="kw">covML</span>(X)</span>
<span id="cb4-2"><a href="#cb4-2"></a><span class="kw">print</span>(S)</span></code></pre></div>
<pre><code>##          A        B       C       D       E       F
## A  0.82772 -0.00637  0.0875 -0.0691  0.0140 -0.2379
## B -0.00637  0.43867 -0.4751 -0.0827  0.1043 -0.1070
## C  0.08755 -0.47507  1.2374  0.1985 -0.1741 -0.1292
## D -0.06905 -0.08272  0.1985  0.9334 -0.2169  0.0553
## E  0.01399  0.10435 -0.1741 -0.2169  0.8998  0.0718
## F -0.23789 -0.10696 -0.1292  0.0553  0.0718  1.0327</code></pre>
<p>Using <code>cov2cor()</code> the well-known correlation matrix could be obtained.</p>
<p>By default, <code>createS()</code> simulates zero-mean i.i.d. normal variables (corresponding to <span class="math inline">\(\Sigma=\Omega=I\)</span> being the identity matrix), but it has plenty of possibilities for more intricate covariance structures. The <code>S</code> matrix could have been obtained directly had we omitted the <code>dataset</code> argument, leaving it to be the default <code>FALSE</code>. The <code>rmvnormal()</code> function is utilized by <code>createS()</code> to generate the normal sample.</p>
<p>We can obtain the precision estimate <code>P</code> using <code>solve()</code> to invert <code>S</code>:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1"></a>P &lt;-<span class="st"> </span><span class="kw">solve</span>(S)</span>
<span id="cb6-2"><a href="#cb6-2"></a><span class="kw">print</span>(P)</span></code></pre></div>
<pre><code>##         A       B       C       D       E       F
## A  1.3084  0.0506 -0.0619  0.0873 -0.0408  0.2971
## B  0.0506  4.3583  1.7199 -0.0747 -0.2472  0.6994
## C -0.0619  1.7199  1.5461 -0.1971  0.0239  0.3662
## D  0.0873 -0.0747 -0.1971  1.1792  0.2609 -0.0936
## E -0.0408 -0.2472  0.0239  0.2609  1.2187 -0.1307
## F  0.2971  0.6994  0.3662 -0.0936 -0.1307  1.1691</code></pre>
<p>That’s it! Everything goes well here only because <span class="math inline">\(n &lt; p\)</span>. However, when <span class="math inline">\(p\)</span> is close to <span class="math inline">\(n\)</span>, the estimate become unstable and varies wildly and when <span class="math inline">\(p\)</span> exceeds <span class="math inline">\(n\)</span> one can no longer invert <span class="math inline">\(S\)</span> and this strategy fails:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1"></a>p &lt;-<span class="st"> </span><span class="dv">25</span></span>
<span id="cb8-2"><a href="#cb8-2"></a>S2 &lt;-<span class="st"> </span><span class="kw">createS</span>(<span class="dt">n =</span> n, <span class="dt">p =</span> p)  <span class="co"># Direct to S</span></span>
<span id="cb8-3"><a href="#cb8-3"></a>P2 &lt;-<span class="st"> </span><span class="kw">solve</span>(S2)</span></code></pre></div>
<pre><code>## Error in solve.default(S2): system is computationally singular: reciprocal condition number = 5.9155e-19</code></pre>
<p>Note that this is now a <span class="math inline">\(25 \times 25\)</span> precision matrix we are trying to estimate. Datasets where <span class="math inline">\(p &gt; n\)</span> are starting to be common, so what now?</p>
<p>To solve the problem, <strong>rags2ridges</strong> adds a so-called ridge penalty to the likelihood above — this method is also called <span class="math inline">\(L_2\)</span> shrinkage and works by “shrinking” the eigenvalues of <span class="math inline">\(S\)</span> in a particular manner to combat that they “explode” when <span class="math inline">\(p \geq n\)</span>.</p>
<p>The core problem that <strong>rags2ridges</strong> solves is that</p>
<p><span class="math display">\[
\ell(\Omega; S) = \ln|\Omega| - \text{tr}(S\Omega) - \frac{\lambda}{2}|| \Omega - T||^2_2
\]</span> where <span class="math inline">\(\lambda &gt; 0\)</span> is the ridge penalty parameter, <span class="math inline">\(T\)</span> is a <span class="math inline">\(p \times p\)</span> known <em>target</em> matrix (which we will get back to) and <span class="math inline">\(||\cdot||_2\)</span> is the <span class="math inline">\(L_2\)</span>-norm. The maximizing solution here is surprisingly on closed form, but it is rather complicated<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>. Assume for now the target matrix is an all zero matrix and thus out of the equation.</p>
<p>The core function of <strong>rags2ridges</strong> is <code>ridgeP</code> which computes this estimate in a fast manner.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1"></a>P2 &lt;-<span class="st"> </span><span class="kw">ridgeP</span>(S2, <span class="dt">lambda =</span> <span class="fl">1.17</span>)</span>
<span id="cb10-2"><a href="#cb10-2"></a><span class="kw">print</span>(P2[<span class="dv">1</span><span class="op">:</span><span class="dv">7</span>, <span class="dv">1</span><span class="op">:</span><span class="dv">7</span>]) <span class="co"># Showing only the 7 first cols and rows</span></span></code></pre></div>
<pre><code>##         A       B       C       D        E        F       G
## A  3.5513 -0.3371  0.1171  0.1337  0.11463  0.05037 -0.0707
## B -0.3371  3.5698  0.3351  0.2159  0.04701 -0.11920 -0.0886
## C  0.1171  0.3351  3.9316 -0.3253 -0.05029 -0.08893 -0.2153
## D  0.1337  0.2159 -0.3253  3.3600  0.19479  0.01609 -0.3689
## E  0.1146  0.0470 -0.0503  0.1948  4.38849  0.00704  0.0729
## F  0.0504 -0.1192 -0.0889  0.0161  0.00704  3.82352 -0.1517
## G -0.0707 -0.0886 -0.2153 -0.3689  0.07289 -0.15170  3.8998</code></pre>
<p>And voilà, we have our estimate. We will now discuss the penalty parameters and target matrix and how to choose them.</p>
</div>
<div id="the-penalty-parameter" class="section level3">
<h3>The penalty parameter</h3>
<p>The penalty parameter <span class="math inline">\(\lambda\)</span> (<code>lambda</code>) shrinks the values of <span class="math inline">\(P\)</span> such toward 0 (when <span class="math inline">\(T = 0\)</span>) — i.e. very larges values of <span class="math inline">\(\lambda\)</span> makes <span class="math inline">\(P\)</span> “small” and more stable whereas smaller values of <span class="math inline">\(\lambda\)</span> makes the <span class="math inline">\(P\)</span> tend toward the (possibly non-existent) <span class="math inline">\(S^{-1}\)</span>. So what <code>lambda</code> should you choose? One strategy for choosing <span class="math inline">\(\lambda\)</span> is selecting it to be stable yet precise (a bias-variance trade-off). Automatic k-fold cross-validation can be done with <code>optPenalty.kCVauto()</code>is well suited for this:</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1"></a>Y &lt;-<span class="st"> </span><span class="kw">createS</span>(n, p, <span class="dt">dataset =</span> <span class="ot">TRUE</span>)</span>
<span id="cb12-2"><a href="#cb12-2"></a>opt &lt;-<span class="st"> </span><span class="kw">optPenalty.kCVauto</span>(Y, <span class="dt">lambdaMin =</span> <span class="fl">0.001</span>, <span class="dt">lambdaMax =</span> <span class="dv">100</span>)</span>
<span id="cb12-3"><a href="#cb12-3"></a><span class="kw">str</span>(opt)</span></code></pre></div>
<pre><code>## List of 2
##  $ optLambda: num 0.79
##  $ optPrec  : &#39;ridgeP&#39; num [1:25, 1:25] 3.1074 -0.0385 -0.1297 -0.0963 -0.1403 ...
##   ..- attr(*, &quot;lambda&quot;)= num 0.79
##   ..- attr(*, &quot;dimnames&quot;)=List of 2
##   .. ..$ : chr [1:25] &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot; ...
##   .. ..$ : chr [1:25] &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot; ...</code></pre>
<p>As seen, the function returns a list with the optimal penalty parameter and corresponding ridge precision estimate. By default, the the functions performs leave-one-out cross validation. See ?optPenalty.kCVauto` for more information.</p>
</div>
<div id="the-target-matrix" class="section level3">
<h3>The target matrix</h3>
<p>The target matrix <span class="math inline">\(T\)</span> is a matrix the same size as <span class="math inline">\(P\)</span> which the estimate is “shrunken” toward — i.e. for large values of <span class="math inline">\(\lambda\)</span> the estimate goes toward <span class="math inline">\(T\)</span>. The choice of the target is another subject. While one might first think that the all-zeros <span class="math inline">\(T = [0]\)</span> would be a default it is intuitively not a good target. This is because we’d like an estimate that is positive definite (the matrix-equivalent to at positive number) and the null-matrix is not positive definite.</p>
<p>If one has a very good prior estimate or some other information this might used to construct the target. E.g. the function <code>kegg.target()</code> utilizes the <em>Kyoto Encyclopedia of Genes and Genomes</em> (KEGG) database of gene and gene-networks together with pilot data to construct a target.</p>
<p>In the absence of such knowledge, the default could be a data-driven diagonal matrix. The function <code>default.target()</code> offers some different approaches to selecting this. A good choice here is often the diagonal matrix times the reciprocal mean of the eigenvalues of the sample covariance as entries. See <code>?default.target</code> for more choices.</p>
</div>
<div id="gaussian-graphical-modeling-and-post-processing" class="section level3">
<h3>Gaussian graphical modeling and post processing</h3>
<blockquote>
<h4 id="what-is-so-interesting-with-the-precision-matrix-anyway-im-always-interested-in-correlations-and-thus-the-correlation-matrix.">What is so interesting with the precision matrix anyway? I’m always interested in correlations and thus the correlation matrix.</h4>
</blockquote>
<p>As you may know, correlation does not imply causation. Nor does covariance imply causation. However, precision matrix provides stronger hints at causation. A relatively simple transformation of <span class="math inline">\(P\)</span> maps it to partial correlations—much like how the sample covariance <span class="math inline">\(S\)</span> easily maps to the correlation matrix. More precisely, the <span class="math inline">\(ij\)</span>th partial correlation is given by</p>
<p><span class="math display">\[
\rho_{ij|\text{all others}} = \frac{- p_{ij}}{\sqrt{p_{ii}p_{jj}}}
\]</span></p>
<p>where <span class="math inline">\(p_{ij}\)</span> is the <span class="math inline">\(ij\)</span>th entry of <span class="math inline">\(P\)</span>.</p>
<p>Partial correlations measure the linear association between two random variables whilst removing the effect of other random variables; in this case, it is all the remaining variables. This somewhat absolves the issue in “regular” correlations where are often correlated but only indirectly; either by sort of ‘transitivity of correlations’ (which does not hold generally and is not<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> so<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a> simple<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a>) or by common underlying variables.</p>
<blockquote>
<h4 id="ok-but-what-is-graphical-about-the-graphical-ridge-estimate">OK, but what is <strong>graphical</strong> about the graphical ridge estimate?</h4>
</blockquote>
<p>In a multivariate normal model, <span class="math inline">\(p_{ij} = p_{ji} = 0\)</span> if and only if <span class="math inline">\(X_i\)</span> and <span class="math inline">\(X_j\)</span> are conditionally independent when condition on all other variables. I.e. <span class="math inline">\(X_i\)</span> and <span class="math inline">\(X_j\)</span> are conditionally independent given all <span class="math inline">\(X_k\)</span> where <span class="math inline">\(k \neq i\)</span> and <span class="math inline">\(k \neq j\)</span> if and when the <span class="math inline">\(ij\)</span>th and <span class="math inline">\({ji}\)</span>th elements of <span class="math inline">\(P\)</span> are zero. In real world applications, this means that <span class="math inline">\(P\)</span> is often relatively sparse (lots of zeros). This also points to the close relationship between <span class="math inline">\(P\)</span> and the partial correlations.</p>
<p>The non-zero entries of the a symmetric PD matrix can them be interpreted the edges of a graph where nodes correspond to the variables.</p>
<blockquote>
<h4 id="graphical-ridge-estimation-why-not-graphical-lasso">Graphical ridge estimation? Why not graphical Lasso?</h4>
</blockquote>
<p>The graphical lasso (gLasso) is the L1-equivalent to graphical ridge. A nice feature of the L1 penalty automatically induces sparsity and thus also select the edges in the underlying graph. The L2 penalty of <em>rags2ridges</em> relies on an extra step that selects the edges after <span class="math inline">\(P\)</span> is estimated. While some may argue this as a drawback (typically due to a lack of perceived simplicity), it is often beneficial to separate the “variable selection” and estimation.</p>
<p>First, a separate post-hoc selection step allows for greater flexibility.</p>
<p>Secondly, when co-linearity is present the L1 penalty is “unstable” in the selection between the items. I.e. if 2 covariances are co-linear only one of them will typically be selected in a unpredictable way whereas the L2 will put equal weight on both and “average” their effect. Ultimately, this means that the L2 estimate is typically more stable than the L1.</p>
<p>At last point to mention here is also that the true underlying graph might not always be very sparse (or sparse at all).</p>
<blockquote>
<h4 id="how-do-i-select-the-edges-then">How do I select the edges then?</h4>
</blockquote>
<p>The <code>sparsify()</code> functions lets you select the non-zero entries of P corresponding to edges. It supports a handful different approaches ranging from simple thresholding to false discovery rate based selection.</p>
<p>After edge select <code>GGMnetworkStats()</code> can be utilized to get summary statistics of the resulting graph topology.</p>
</div>
</div>
<div id="concluding-remarks" class="section level2">
<h2>Concluding remarks</h2>
<p>The <code>fullMontyS()</code> function is a convenience wrapper getting from the data through the penalized estimate to the corresponding conditional independence graph and topology summaries.</p>
<p>For a full introduction to the theoretical properties as well as more context to the problem, see <a href="https://www.sciencedirect.com/science/article/pii/S0167947316301141">van Wieringen &amp; Peeters (2016)</a>.</p>
<p><strong>rags2ridges</strong> also comes with functionality for <em>targeted</em> and <em>grouped</em> (or, <em>fused</em>) graphical ridge regression called the fused graphical ridge. See <a href="https://arxiv.org/abs/1509.07982">[2]</a> below. The functions in this <code>rags2ridges</code> module are generally post-fixed with <code>.fused</code>.</p>
<div id="references" class="section level3">
<h3>References</h3>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0167947316301141">1.</a>. van Wieringen, W.N. and Peeters, C.F.W. <strong>(2016)</strong>. <a href="https://www.sciencedirect.com/science/article/pii/S0167947316301141"><em>Ridge Estimation of Inverse Covariance Matrices from High-Dimensional Data.</em></a> Computational Statistics &amp; Data Analysis, vol. 103: 284-303.</p>
<p><a href="https://arxiv.org/abs/1509.07982">2.</a> Bilgrau, A.E., Peeters, C.F.W., Eriksen, P.S., Boegsted, M., and van Wieringen, W.N. <strong>(2015)</strong>. <a href="https://arxiv.org/abs/1509.07982"><em>Targeted Fused Ridge Estimation of Inverse Covariance Matrices from Multiple High-Dimensional Data Classes.</em></a> arXiv:1509.07982 [stat.ME].</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Solution for the graphical ridge problem: <span class="math display">\[
P(\lambda) 
= \Bigg\{ \bigg[ \lambda I_{p\times p} + \frac{1}{4}(S - \lambda T)^{2} \bigg]^{1/2} + \frac{1}{2}(S -\lambda T) \Bigg\}^{-1}
\]</span><a href="#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p><a href="https://stats.stackexchange.com/questions/181376/is-correlation-transitive" class="uri">https://stats.stackexchange.com/questions/181376/is-correlation-transitive</a><a href="#fnref2" class="footnote-back">↩︎</a></p></li>
<li id="fn3"><p><a href="https://emilkirkegaard.dk/en/2016/02/causality-transitivity-and-correlation/" class="uri">https://emilkirkegaard.dk/en/2016/02/causality-transitivity-and-correlation/</a><a href="#fnref3" class="footnote-back">↩︎</a></p></li>
<li id="fn4"><p><a href="https://terrytao.wordpress.com/2014/06/05/when-is-correlation-transitive/" class="uri">https://terrytao.wordpress.com/2014/06/05/when-is-correlation-transitive/</a><a href="#fnref4" class="footnote-back">↩︎</a></p></li>
</ol>
</div>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
